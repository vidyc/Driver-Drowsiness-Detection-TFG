{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openvino.runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\TFG\\MVP\\NTHUDDD_notebook.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtest_environment\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtest_environment\u001b[39;00m \u001b[39mimport\u001b[39;00m TestEnvironment\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39myaml\u001b[39;00m\n",
      "File \u001b[1;32md:\\TFG\\MVP\\test_environment.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/test_environment.py?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/test_environment.py?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msetuptools\u001b[39;00m \u001b[39mimport\u001b[39;00m sic\n\u001b[1;32m----> <a href='file:///d%3A/TFG/MVP/test_environment.py?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minference\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39minf\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/test_environment.py?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmetrics_obtention\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmo\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/test_environment.py?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_all_videos_from_directory\u001b[39m(directory: \u001b[39mstr\u001b[39m, videos\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "File \u001b[1;32md:\\TFG\\MVP\\inference.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/TFG/MVP/inference.py?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/TFG/MVP/inference.py?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/TFG/MVP/inference.py?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mregion_detection\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mroi\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/TFG/MVP/inference.py?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimage_analysis\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/TFG/MVP/inference.py?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmetrics_obtention\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmo\u001b[39;00m\n",
      "File \u001b[1;32md:\\TFG\\MVP\\region_detection.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/region_detection.py?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/region_detection.py?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> <a href='file:///d%3A/TFG/MVP/region_detection.py?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenvino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mov\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/region_detection.py?line=5'>6</a>\u001b[0m \u001b[39m# import dlib\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/TFG/MVP/region_detection.py?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openvino.runtime'"
     ]
    }
   ],
   "source": [
    "import test_environment as t\n",
    "from test_environment import TestEnvironment\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.loader.FullLoader)\n",
    "video = cv2.VideoCapture(\"images/NTHUDDD_dataset/Training_Evaluation_Dataset/Training Dataset/036/noglasses/nonsleepyCombination.avi\")\n",
    "valid, img = video.read()\n",
    "for i in range(0, 395):\n",
    "    valid, img = video.read()\n",
    "\n",
    "modelFile = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"deploy.prototxt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "h, w = img.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0,\n",
    "(300, 300), (104.0, 117.0, 123.0))\n",
    "net.setInput(blob)\n",
    "faces = net.forward()\n",
    "#to draw faces on image\n",
    "for i in range(faces.shape[2]):\n",
    "    confidence = faces[0, 0, i, 2]\n",
    "    if confidence > 0.5:\n",
    "        box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (x, y, x1, y1) = box.astype(\"int\")\n",
    "        cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow(\"\", img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotation_box(img, rotation_vector, translation_vector, camera_matrix, color=(255, 255, 0), line_width=2):\n",
    "    \"\"\"Draw a 3D box as annotation of pose\"\"\"\n",
    "    point_3d = []\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "    rear_size = 1\n",
    "    rear_depth = 0\n",
    "    point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "    point_3d.append((-rear_size, rear_size, rear_depth))\n",
    "    point_3d.append((rear_size, rear_size, rear_depth))\n",
    "    point_3d.append((rear_size, -rear_size, rear_depth))\n",
    "    point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "\n",
    "    front_size = img.shape[1]\n",
    "    front_depth = front_size*2\n",
    "    point_3d.append((-front_size, -front_size, front_depth))\n",
    "    point_3d.append((-front_size, front_size, front_depth))\n",
    "    point_3d.append((front_size, front_size, front_depth))\n",
    "    point_3d.append((front_size, -front_size, front_depth))\n",
    "    point_3d.append((-front_size, -front_size, front_depth))\n",
    "    point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)\n",
    "\n",
    "    # Map to 2d img points\n",
    "    (point_2d, _) = cv2.projectPoints(point_3d,\n",
    "                                      rotation_vector,\n",
    "                                      translation_vector,\n",
    "                                      camera_matrix,\n",
    "                                      dist_coeffs)\n",
    "    point_2d = np.int32(point_2d.reshape(-1, 2))\n",
    "    \n",
    "\n",
    "    # # Draw all the lines\n",
    "    # cv2.polylines(img, [point_2d], True, color, line_width, cv2.LINE_AA)\n",
    "    k = (point_2d[5] + point_2d[8])//2\n",
    "    # cv2.line(img, tuple(point_2d[1]), tuple(\n",
    "    #     point_2d[6]), color, line_width, cv2.LINE_AA)\n",
    "    # cv2.line(img, tuple(point_2d[2]), tuple(\n",
    "    #     point_2d[7]), color, line_width, cv2.LINE_AA)\n",
    "    # cv2.line(img, tuple(point_2d[3]), tuple(\n",
    "    #     point_2d[8]), color, line_width, cv2.LINE_AA)\n",
    "    \n",
    "    return(point_2d[2], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import region_detection as roi\n",
    "import math\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.loader.FullLoader)\n",
    "video = cv2.VideoCapture(\"images/adrian/head_pose.mp4\")\n",
    "# video = cv2.VideoCapture(\"images/NTHUDDD_dataset/Training_Evaluation_Dataset/Training Dataset/036/noglasses/nonsleepyCombination.avi\")\n",
    "valid, img = video.read()\n",
    "# for i in range(0, 392):\n",
    "#     valid, img = video.read()\n",
    "while valid:\n",
    "    size = img.shape\n",
    "    faces, frame = roi.mediapipe_face_mesh(img, debug=False)\n",
    "    \n",
    "    print(\"ENTER\")\n",
    "    if faces is None or faces.multi_face_landmarks is None:\n",
    "        valid, img = video.read()\n",
    "        continue\n",
    "    print(\"NOT GOING\")\n",
    "\n",
    "    landmarks = faces.multi_face_landmarks[0]\n",
    "    \n",
    "    points_2D_indexes = [1, 152, 263, 33, 291, 61]\n",
    "    points_2D = np.array([(landmarks.landmark[index].x * size[1], landmarks.landmark[index].y * size[0]) for index in points_2D_indexes])\n",
    "    points_3D = np.array([(landmarks.landmark[index].x * size[1], landmarks.landmark[index].y * size[0], landmarks.landmark[index].z) for index in points_2D_indexes])\n",
    "\n",
    "    dist_coeffs = np.zeros((4,1))  \n",
    "    focal_length = size[1]\n",
    "    center = (size[1]/2, size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "                            [[focal_length, 0, center[0]],\n",
    "                            [0, focal_length, center[1]],\n",
    "                            [0, 0, 1]], dtype = \"double\"\n",
    "                            )\n",
    "\n",
    "    points_3D = np.array([\n",
    "                (0.0, 0.0, 0.0),       #Nose ti\n",
    "                (0.0, -330.0, -65.0),  #Chin\n",
    "                (-225.0, 170.0, -135.0),#Left eye corner\n",
    "                (225.0, 170.0, -135.0), #Right eye corner \n",
    "                (-150.0, -150.0, -125.0),#Left mouth \n",
    "                (150.0, -150.0, -125.0) #Right mouth \n",
    "                ])     \n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(points_3D, points_2D, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    nose_end_point2D, jacobian = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "    for p in points_2D:\n",
    "        cv2.circle(img, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\n",
    "\n",
    "    p1 = ( int(points_2D[0][0]), int(points_2D[0][1]))\n",
    "    \n",
    "    p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "    x1, x2 = draw_annotation_box(img, rotation_vector, translation_vector, camera_matrix)\n",
    "    \n",
    "    cv2.line(img, p1, p2, (255,255,255), 2)\n",
    "    #cv2.line(img, tuple(x1), tuple(x2), (255, 255, 0), 2)\n",
    "    try:\n",
    "        m = (p2[1] - p1[1])/(p2[0] - p1[0])\n",
    "        ang1 = int(math.degrees(math.atan(m)))\n",
    "\n",
    "        x = p2[0] - p1[0]\n",
    "        y = p2[1] - p1[1]\n",
    "        z = 1 - 0 # asumimos que p1[z] = 0\n",
    "        pitch = int(math.degrees(math.acos(y / math.sqrt(x**2 + y**2 + z**2)))) - 90  # shiftamos 90 grados para que recto = 0\n",
    "        yaw = int(math.degrees(math.acos(x / math.sqrt(x**2 + y**2 + z**2))))  # shiftamos 90 grados para que recto = 0\n",
    "    except:\n",
    "        ang1 = 90\n",
    "        \n",
    "    try:\n",
    "        m = (x2[1] - x1[1])/(x2[0] - x1[0])\n",
    "        x = x2[0] - x1[0]\n",
    "        y = x2[1] - x1[1]\n",
    "        z = 1 - 0 # asumimos que p1[z] = 0\n",
    "        ang2 = int(math.degrees(math.atan(-1/m)))\n",
    "        yaw = int(math.degrees(math.acos(x / math.sqrt(x**2 + y**2 + z**2)))) - 90 # shiftamos 90 grados para que recto = 0\n",
    "    except:\n",
    "        ang2 = 90\n",
    "    # Display image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    cv2.putText(img, str(ang1), tuple(p1), font, 2, (128, 255, 255), 3)\n",
    "    cv2.putText(img, str(pitch), tuple((100, 100)), font, 2, (0, 255, 0), 3)\n",
    "    cv2.putText(img, str(yaw), tuple((100, 200)), font, 2, (0, 255, 0), 3)\n",
    "    cv2.putText(img, str(ang2), tuple((100, 500)), font, 2, (255, 255, 128), 3)\n",
    "    cv2.imshow('', img)\n",
    "    cv2.waitKey()\n",
    "    valid, img = video.read()\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\TFG\\MVP\\NTHUDDD_notebook.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000005?line=12'>13</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39ma2\u001b[39m\u001b[39m\"\u001b[39m, equalized)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000005?line=13'>14</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000005?line=15'>16</a>\u001b[0m frame_metrics, image, _ \u001b[39m=\u001b[39m mo\u001b[39m.\u001b[39mprocess_frame(image, config[\u001b[39m\"\u001b[39m\u001b[39mmetric_obtention\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000005?line=16'>17</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000005?line=17'>18</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mo' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "equalized = clahe.apply(gray_image)\n",
    "\n",
    "cv2.imshow(\"\", image)\n",
    "cv2.imshow(\"a\", gray_image)\n",
    "cv2.imshow(\"a2\", equalized)\n",
    "cv2.waitKey()\n",
    "\n",
    "frame_metrics, image, _ = mo.process_frame(image, config[\"metric_obtention\"])\n",
    "cv2.imshow(' ', image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = TestEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.loader.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_and_labels = (test_env.get_videos_and_labels_NTHUDDD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (3330)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\TFG\\MVP\\NTHUDDD_notebook.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmetrics_obtention\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmo\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/TFG/MVP/NTHUDDD_notebook.ipynb#ch0000003?line=1'>2</a>\u001b[0m df_list \u001b[39m=\u001b[39m mo\u001b[39m.\u001b[39;49mcreate_dataset_from_videos_NTHU(videos_and_labels, target_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mNTHUDDD_dataset/\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmetric_obtention\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32md:\\TFG\\MVP\\metrics_obtention.py:300\u001b[0m, in \u001b[0;36mcreate_dataset_from_videos_NTHU\u001b[1;34m(videos_and_labels, target_folder, config)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=297'>298</a>\u001b[0m labels \u001b[39m=\u001b[39m vid_lab[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=298'>299</a>\u001b[0m video \u001b[39m=\u001b[39m vid_lab[\u001b[39m\"\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=299'>300</a>\u001b[0m video_df \u001b[39m=\u001b[39m create_dataset_from_video(video, subject, config, labels)\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=300'>301</a>\u001b[0m video_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtarget_folder\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00msubject\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mscenario\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mvideo_type\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=301'>302</a>\u001b[0m df_list\u001b[39m.\u001b[39mappend(video_df)\n",
      "File \u001b[1;32md:\\TFG\\MVP\\metrics_obtention.py:288\u001b[0m, in \u001b[0;36mcreate_dataset_from_video\u001b[1;34m(input_video, subject, config, labels)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=284'>285</a>\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(metric_list)\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=286'>287</a>\u001b[0m metric_dataframe[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m labels\n\u001b[1;32m--> <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=287'>288</a>\u001b[0m metric_dataframe[\u001b[39m\"\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [fps] \u001b[39m*\u001b[39m num_samples\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=288'>289</a>\u001b[0m metric_dataframe[\u001b[39m\"\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [subject] \u001b[39m*\u001b[39m num_samples\n\u001b[0;32m    <a href='file:///d%3A/TFG/MVP/metrics_obtention.py?line=289'>290</a>\u001b[0m \u001b[39mreturn\u001b[39;00m metric_dataframe\n",
      "File \u001b[1;32mc:\\Users\\adrim\\.conda\\envs\\TFG\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3651'>3652</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3652'>3653</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3653'>3654</a>\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3654'>3655</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\adrim\\.conda\\envs\\TFG\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3821'>3822</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3822'>3823</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3823'>3824</a>\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3824'>3825</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3829'>3830</a>\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3830'>3831</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3831'>3832</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3833'>3834</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3834'>3835</a>\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3835'>3836</a>\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3836'>3837</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3837'>3838</a>\u001b[0m     ):\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3838'>3839</a>\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=3839'>3840</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\adrim\\.conda\\envs\\TFG\\lib\\site-packages\\pandas\\core\\frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=4531'>4532</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=4533'>4534</a>\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=4534'>4535</a>\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/frame.py?line=4535'>4536</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\adrim\\.conda\\envs\\TFG\\lib\\site-packages\\pandas\\core\\common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=552'>553</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=553'>554</a>\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=554'>555</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=555'>556</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=556'>557</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=557'>558</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=558'>559</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=559'>560</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=560'>561</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/adrim/.conda/envs/TFG/lib/site-packages/pandas/core/common.py?line=561'>562</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (3330)"
     ]
    }
   ],
   "source": [
    "import metrics_obtention as mo\n",
    "df_list = mo.create_dataset_from_videos_NTHU(videos_and_labels, target_folder=\"NTHUDDD_dataset/\", config=config[\"metric_obtention\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa8fa589b840ce477995b34707d4fc5a2b1b0e447bc3d51a6c8914538bfacf38"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('TFG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
