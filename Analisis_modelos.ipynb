{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALISIS 001 DE NTHUDDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import chain, combinations\n",
    "\n",
    "metricas_analisis = [\n",
    "    \"frame_count\", \"perclos\", \"mean_ear\", \"current_time_closed_eyes\", \"blinks_per_minute\",\n",
    "    \"yawns_per_minute\", \"head_nods_per_minute\", \"mean_blink_time\", \"head_nod\", \"yawn\", \"open_eyes\", \"drowsiness\",\n",
    "    \"ear\", \"pitch\", \"yaw\", \"mar\", \"yawn_frequency\", \"head_nod_frequency\", \"blink_frequency\"\n",
    "]\n",
    "# source_folder = \"NTHUDDD_dataset2_step2_600/\"\n",
    "source_folder = \"NTHUDDD_dataset2_nuevosmetodos/\"\n",
    "train_path = f\"{source_folder}train/\"\n",
    "test_path = f\"{source_folder}test/\"\n",
    "label = \"drowsiness\"\n",
    "train_df_list = []\n",
    "test_df_list = []\n",
    "for filename in os.listdir(train_path):\n",
    "    if \"night\" not in filename:\n",
    "        file = os.path.join(train_path, filename)\n",
    "        if os.path.isfile(file) and filename[-4:] == \".csv\" and filename != \"big_df.csv\":\n",
    "            df = pd.read_csv(file)[metricas_analisis]\n",
    "\n",
    "            file_data = filename.split(\"_\")\n",
    "            df[\"subject\"] = filename[:3]\n",
    "            df[\"scenario\"] = file_data[1]\n",
    "            df[\"state\"] = file_data[2][:-4]\n",
    "            df[\"id\"] = df['subject'] + df['scenario'] + df['state']\n",
    "            train_df_list.append(df)\n",
    "\n",
    "for filename in os.listdir(test_path):\n",
    "    if \"night\" not in filename:\n",
    "        file = os.path.join(test_path, filename)\n",
    "        if os.path.isfile(file) and filename[-4:] == \".csv\" and filename != \"big_df.csv\":\n",
    "            df = pd.read_csv(file)[metricas_analisis]\n",
    "            \n",
    "            file_data = filename.split(\"_\")\n",
    "            df[\"subject\"] = filename[:3]\n",
    "            df[\"scenario\"] = file_data[1]\n",
    "            df[\"state\"] = file_data[2][:-4]\n",
    "            df[\"id\"] = df['subject'] + df['scenario'] + df['state']\n",
    "            test_df_list.append(df)\n",
    "train_big_df = pd.concat(train_df_list)\n",
    "test_big_df = pd.concat(test_df_list)\n",
    "# big_df = big_df.drop(\"Unnamed: 0.1\", axis=1)\n",
    "# train_big_df.head()\n",
    "test_big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_big_df.to_csv(\"NTHUDDD_dataset2/big_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_big_df = train_big_df.copy()\n",
    "filtered_test_big_df = test_big_df.copy()\n",
    "# filtered_train_big_df = filtered_train_big_df[filtered_train_big_df[\"frame_count\"] > 200]\n",
    "filtered_train_big_df = filtered_train_big_df[filtered_train_big_df[\"scenario\"] == \"noglasses\"]\n",
    "filtered_train_big_df = filtered_train_big_df[filtered_train_big_df[\"subject\"].isin([\"001\", \"008\", \"009\", \"013\", \"023\", \"034\", \"035\", \"036\"]) ]\n",
    "# filtered_big_df = filtered_big_df[filtered_big_df[\"subject\"].isin([\"011\"]) ]\n",
    "\n",
    "# filtered_test_big_df = filtered_test_big_df[filtered_test_big_df[\"frame_count\"] > 200]\n",
    "filtered_test_big_df = filtered_test_big_df[filtered_test_big_df[\"scenario\"] == \"noglasses\"]\n",
    "# filtered_test_big_df = filtered_test_big_df[filtered_test_big_df[\"subject\"].isin([\"011\", \"016\", \"021\"])]\n",
    "# filtered_test_big_df = filtered_test_big_df[filtered_test_big_df[\"subject\"].isin([\"003\", \"010\", \"011\", \"014\", \"016\", \"017\", \"018\", \"019\", \"021\", \"025\", \"027\", \"028\", \"029\", \"037\"])]\n",
    "filtered_train_big_df.groupby([\"subject\", label]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NUM NULL ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TOTAL NUM ROWS: {filtered_train_big_df.shape[0]}\")\n",
    "print(f\"NUM NULL ROWS: {filtered_train_big_df.shape[0] - filtered_train_big_df.dropna().shape[0]}\")\n",
    "train_big_df_nonull = filtered_train_big_df.dropna().sample(frac=1)\n",
    "train_big_df_nonull[\"open_eyes\"] = train_big_df_nonull[\"open_eyes\"].astype(\"int\")\n",
    "train_big_df_nonull[\"head_nod\"] = train_big_df_nonull[\"head_nod\"].astype(\"int\")\n",
    "train_big_df_nonull[\"yawn\"] = train_big_df_nonull[\"yawn\"].astype(\"int\")\n",
    "\n",
    "\n",
    "print(f\"TOTAL NUM ROWS: {filtered_test_big_df.shape[0]}\")\n",
    "print(f\"NUM NULL ROWS: {filtered_test_big_df.shape[0] - filtered_test_big_df.dropna().shape[0]}\")\n",
    "test_big_df_nonull = filtered_test_big_df.dropna().sample(frac=1)\n",
    "test_big_df_nonull[\"open_eyes\"] = test_big_df_nonull[\"open_eyes\"].astype(\"int\")\n",
    "test_big_df_nonull[\"head_nod\"] = test_big_df_nonull[\"head_nod\"].astype(\"int\")\n",
    "test_big_df_nonull[\"yawn\"] = test_big_df_nonull[\"yawn\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 1: Exploracion del mejor conjunto de metricas\n",
    "\n",
    "La idea es construir el experimento para que con una simple ejecuciÃ³n obtengamos un modelo por cada posible combinacion de metricas y sus respectivos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_posibles = [\n",
    "    \"perclos\", \"current_time_closed_eyes\", \"ear\", \"mar\", \"pitch\",\n",
    "    \"yawns_per_minute\", \"head_nods_per_minute\", \"mean_blink_time\"\n",
    "]\n",
    "experiment = False\n",
    "tamano_minimo_combinacion = 3\n",
    "tipo_modelo = \"lgb\"\n",
    "experiment_folder = \"model_experiments/experiment1/\"\n",
    "numero_metricas = len(metricas_posibles)\n",
    "performance_results = []\n",
    "\n",
    "combinacion_metricas = list(chain.from_iterable(combinations(metricas_posibles, size) for size in range(tamano_minimo_combinacion, numero_metricas + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment:\n",
    "    for ind, combinacion in enumerate(combinacion_metricas):\n",
    "        print(ind)\n",
    "        # model_features = [\"mean_blink_time\", \"perclos\", \"current_time_closed_eyes\", \"yawns_per_minute\", \"head_nods_per_minute\"]\n",
    "        model_features = list(combinacion)\n",
    "        x_train = train_big_df_nonull[model_features]\n",
    "        y_train = train_big_df_nonull[label]\n",
    "        x_test = test_big_df_nonull[model_features]\n",
    "        y_test = test_big_df_nonull[label]\n",
    "        \n",
    "        if tipo_modelo == \"lgb\":\n",
    "            model = lgb.LGBMClassifier(\n",
    "                boosting_type=\"dart\",\n",
    "                num_leaves=400,\n",
    "                num_iterations=1000,\n",
    "                learning_rate=0.01,\n",
    "                verbosity=-1,\n",
    "                #early_stopping=20,\n",
    "            )\n",
    "            file = os.path.join(experiment_folder, f\"lgb_model_{ind}.joblib\")\n",
    "        elif mod == \"svm\":\n",
    "            model = svm.SVC(kernel=\"linear\")\n",
    "            file = os.path.join(experiment_folder, f\"svm_model_{ind}.joblib\")\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        dump(model, file)\n",
    "        y_pred = model.predict(x_train)\n",
    "        train_res = classification_report(y_train, y_pred)\n",
    "        y_pred2 = model.predict(x_test)\n",
    "        test_res = classification_report(y_test, y_pred2)\n",
    "        \n",
    "        print(train_res)\n",
    "        print(test_res)\n",
    "\n",
    "        performance_results.append((model_features, train_res, test_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment:\n",
    "    best_accuracy = -1\n",
    "    best_ind = -1\n",
    "    ind = 0\n",
    "    for model_features, train_res, test_res in performance_results:\n",
    "        # print(test_res)\n",
    "        split_test_res = test_res.split()\n",
    "        # print(split_test_res)\n",
    "        accuracy = float(split_test_res[15])\n",
    "        if accuracy >= best_accuracy:\n",
    "            best_ind = ind\n",
    "            best_accuracy = accuracy\n",
    "        ind += 1\n",
    "\n",
    "    print(f\"BEST_IND: {best_ind} {performance_results[best_ind][0]} --> ACC: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escribimos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment:\n",
    "    file_path = os.path.join(experiment_folder, \"report.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "    file = open(file_path, \"a\")\n",
    "    for model_features, train_res, test_res in performance_results:\n",
    "        file.write(f\"{str(model_features)}\\n\")\n",
    "        file.write(train_res)\n",
    "        file.write(test_res)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = [\"yaw\", \"ear\", \"mar\", \"pitch\", 'perclos', 'current_time_closed_eyes', \"blinks_per_minute\", \"mean_blink_time\", \"yawns_per_minute\", \"head_nods_per_minute\"]\n",
    "x_train = train_big_df_nonull[model_features]\n",
    "y_train = train_big_df_nonull[label]\n",
    "x_test = test_big_df_nonull[model_features]\n",
    "y_test = test_big_df_nonull[label]\n",
    "\n",
    "x_test[\"yaw\"] = abs(x_test[\"yaw\"])\n",
    "\n",
    "# test_size = len(labels)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(x_train)\n",
    "\n",
    "x_train_normalized = min_max_scaler.transform(x_train)\n",
    "x_test_normalized = min_max_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit(x_train))\n",
    "print(scaler.mean_)\n",
    "\n",
    "x_train_transformed = scaler.transform(x_train)\n",
    "x_test_transformed = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"\"\n",
    "x_train_data = x_train\n",
    "x_test_data = x_test \n",
    "if data_type == \"normalized\":\n",
    "    x_train_data = x_train_normalized\n",
    "    x_test_data = x_test_normalized\n",
    "elif data_type == \"standardized\":\n",
    "    x_train_data = x_train_transformed\n",
    "    x_test_data = x_test_normalized\n",
    "\n",
    "mod = \"lgb\"\n",
    "if mod == \"lgb\":\n",
    "    model = lgb.LGBMClassifier(\n",
    "        boosting_typ=\"gbdt\",\n",
    "        num_leaves=1820,\n",
    "        num_iterations=1000,\n",
    "        n_estimators=10000,\n",
    "        max_depth=9,\n",
    "        min_data_in_leaf=200,\n",
    "        lambda_l1=100,\n",
    "        lambda_l2=50,\n",
    "        min_split_gain=6.23386152668858,\n",
    "        bagging_fraction=0.2,\n",
    "        baggin_freq=1,\n",
    "        feature_fraction=0.9,\n",
    "        learning_rate=0.2877383794743792,\n",
    "        verbosity=-1,\n",
    "        #early_stopping=20,\n",
    "        importance_type='gain',\n",
    "    )\n",
    "    # model.fit(x_train_data, y_train, feature_name=model_features)\n",
    "    file = \"lgb_models/lgb_model_0.joblib\"\n",
    "elif mod == \"svm\":\n",
    "    model = svm.SVC(kernel=\"linear\")\n",
    "    file = \"svm_model_0.joblib\"\n",
    "elif mod == \"knn\":\n",
    "    model = KNeighborsClassifier(n_neighbors=30)\n",
    "    file = \"knn_model_0.joblib\"\n",
    "elif mod == \"reg\":\n",
    "    model = LogisticRegression(random_state=0, solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0)\n",
    "    file = \"logreg_model_0.joblib\"\n",
    "\n",
    "# eval_set=[(x_test_data, y_test)], callbacks=[lgb.early_stopping(stopping_rounds=5)]\n",
    "model.fit(x_train_data, y_train)\n",
    "dump(model, file)\n",
    "y_pred = model.predict(x_train_data)\n",
    "print(classification_report(y_train, y_pred))\n",
    "y_pred2 = model.predict(x_test_data)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "model.predict(x_test.iloc[5:105])\n",
    "t = time.time() - s\n",
    "print(t)\n",
    "print(t / 100)\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_model = load(\"lgb_model_optuna.joblib\")\n",
    "y_pred_proba = np.transpose(model.predict_proba(x_test))\n",
    "print(y_pred_proba)\n",
    "# y_pred = [ 0 if x0 > 0.5 else 1 for x0, x1 in y_pred_proba ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred2 = model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba[0], pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba[1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba[0], pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba[1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "# model = load(\"lgb_models/lgb_model_0.joblib\")\n",
    "# previous_model = load(\"lgb_model_003.joblib\")\n",
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_importance = x_test\n",
    "shap_values = explainer.shap_values(x_importance)\n",
    "# print(len(shap_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({'Value': model.feature_importances_, 'Feature': x_test.columns}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], x_importance, plot_type=\"dot\", plot_size=(14, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"blink_frequency\", shap_values[0], x_importance, interaction_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': x_test.columns})\n",
    "plt.figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale = 5)\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                    ascending=False)[0:20])\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('lgbm_importances-01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 1311\n",
    "pd_df = np.asarray(x_test.iloc[[row]])\n",
    "# pd_df = np.array([[0.24, 0.08, (2/30), 0.0, 0]])\n",
    "shap_values = explainer.shap_values(pd_df)\n",
    "expected_value = explainer.expected_value\n",
    "\n",
    "shap_values0 = shap_values[0]\n",
    "shap_values1 = shap_values[1]\n",
    "expected_value0 = expected_value[0]\n",
    "expected_value1 = expected_value[1]\n",
    "\n",
    "print(np.asarray(y_test.iloc[[row]]))\n",
    "print(model.predict(pd_df))\n",
    "print(shap_values)\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.columns)\n",
    "print(pd_df)\n",
    "shap.summary_plot(shap_values0, features=pd_df, feature_names=x_test.columns, plot_type=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(expected_value0, shap_values0, features=pd_df, feature_names=list(x_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis error del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_big_df_nonull.groupby([\"subject\", label]).mean()[metricas_posibles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join test_data and predictions\n",
    "\n",
    "test_with_preds_labels = test_big_df_nonull.copy()\n",
    "test_with_preds_labels[\"pred\"] = y_pred2\n",
    "test_with_preds_labels[\"hit\"] = test_with_preds_labels[\"pred\"] == test_with_preds_labels[label]\n",
    "\n",
    "print(test_with_preds_labels[label])\n",
    "print(y_test)\n",
    "\n",
    "test_with_preds_labels.groupby([\"id\", label])[[\"hit\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna hiperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from sklearn.metrics import log_loss\n",
    "def train_lgb_optuna(x_train, y_train, x_test, y_test, features, save=False):\n",
    "    def objective(trial, x_train, y_train, x_test, y_test):\n",
    "        param_grid = {\n",
    "            \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "            \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [1000, 10000, 100000]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 40),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "            \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "            \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "            \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 25),\n",
    "            \"bagging_fraction\": trial.suggest_float(\n",
    "                \"bagging_fraction\", 0.2, 0.95, step=0.1\n",
    "            ),\n",
    "            \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1, 2, 3]),\n",
    "            \"feature_fraction\": trial.suggest_float(\n",
    "                \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        model = lgb.LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            eval_set=[(x_test, y_test)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            early_stopping_rounds=300,\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        preds = model.predict_proba(x_test)\n",
    "        score = log_loss(y_test, preds)\n",
    "        print(score)\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "    func = lambda trial: objective(trial, x_train, y_train, x_test, y_test)\n",
    "    study.optimize(func, n_trials=3000)\n",
    "\n",
    "    best_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"seed\": 42\n",
    "    } \n",
    "    best_params.update(study.best_params)\n",
    "    print(f\"\\tBest value (binary_logloss): {study.best_value:.5f}\")\n",
    "    print(f\"\\tBest params:\")\n",
    "\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"\\t\\t{key}: {value}\")\n",
    "    \n",
    "    # x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "    train_dataset = lgb.Dataset(\n",
    "        data=x_train,\n",
    "        label=y_train,\n",
    "    )\n",
    "    lgb_model = lgb.train(best_params,\n",
    "                          train_dataset,\n",
    "                          num_boost_round=4000,\n",
    "                          ) \n",
    "\n",
    "    y_pred = [round(pred) for pred in lgb_model.predict(x_train)]\n",
    "    print(classification_report(y_train, y_pred))  \n",
    "    y_pred = [round(pred) for pred in lgb_model.predict(x_test)]\n",
    "    print(classification_report(y_test, y_pred))   \n",
    "\n",
    "    if save:\n",
    "        dump(lgb_model, \"lgb_model_optuna.joblib\")\n",
    "    return lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb_optuna(x_train, y_train, x_test, y_test, [], save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_big_df_nonull.groupby([\"subject\", label]).mean()[metricas_posibles + [\"open_eyes\", \"head_nod\", \"yawn\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa8fa589b840ce477995b34707d4fc5a2b1b0e447bc3d51a6c8914538bfacf38"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('TFG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
